{"cmd": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\ndef generate_synthetic_data(num_samples=1000):\n    \"\"\"Generate synthetic data x and true latent variables z.\"\"\"\n    z = torch.randn(num_samples, 2)  # True latent variable (Gaussian)\n    x = z + 0.1 * torch.randn_like(z)  # Observed data with noise\n    return x, z\n\n\nclass VariationalPosterior(nn.Module):\n    \"\"\"Approximate posterior q_phi(z|x).\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 4),  # Mean and log-variance\n        )\n\n    def forward(self, x):\n        params = self.encoder(x)\n        mean, log_var = params[:, :2], params[:, 2:]\n        std = torch.exp(0.5 * log_var)\n        return mean, std\n\n    def sample(self, x):\n        mean, std = self.forward(x)\n        eps = torch.randn_like(mean)\n        return mean + eps * std\n\n\nclass Discriminator(nn.Module):\n    \"\"\"Discriminator T_psi(x, z).\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(4, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, z):\n        return self.net(torch.cat([x, z], dim=-1))\n\n\nclass AdaptiveContrast(nn.Module):\n    \"\"\"Adaptive Contrast Module.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, z_fake, z_real):\n        \"\"\"\n        Computes adaptive contrast loss.\n\n        z_fake: Samples from the approximate posterior.\n        z_real: Samples from the true posterior.\n\n        Returns:\n            contrast_loss: Adaptive contrast loss.\n        \"\"\"\n        z_fake_mean = z_fake.mean(dim=0)\n        z_real_mean = z_real.mean(dim=0)\n        contrast_loss = torch.norm(z_fake_mean - z_real_mean, p=2)\n        return contrast_loss\n\n\n# Generate synthetic data\nx_data, z_true = generate_synthetic_data()\n\n# Models\nq_phi = VariationalPosterior()\nT_psi = Discriminator()\nadaptive_contrast = AdaptiveContrast()\n\n# Optimizers\noptimizer_q = optim.Adam(q_phi.parameters(), lr=1e-3)\noptimizer_t = optim.Adam(T_psi.parameters(), lr=1e-3)\n\n# Training loop\nepochs = 500\nbatch_size = 64\nfor epoch in range(epochs):\n    for i in range(0, x_data.size(0), batch_size):\n        x_batch = x_data[i : i + batch_size]\n\n        # --- Train Discriminator ---\n        z_fake = q_phi.sample(x_batch)  # Samples from approximate posterior\n        z_real = z_true[i : i + batch_size]  # Samples from true posterior\n\n        logits_real = T_psi(x_batch, z_real)\n        logits_fake = T_psi(x_batch, z_fake)\n\n        loss_t = -torch.mean(\n            torch.log(logits_real + 1e-8) + torch.log(1 - logits_fake + 1e-8)\n        )\n\n        optimizer_t.zero_grad()\n        loss_t.backward()\n        optimizer_t.step()\n\n        # --- Train Generator ---\n        z_fake = q_phi.sample(x_batch)\n        logits_fake = T_psi(x_batch, z_fake)\n\n        loss_q = -torch.mean(torch.log(logits_fake + 1e-8))\n\n        # Add Adaptive Contrast Loss\n        contrast_loss = adaptive_contrast(z_fake, z_real)\n        total_loss_q = loss_q + 0.1 * contrast_loss  # Weighted combination\n\n        optimizer_q.zero_grad()\n        total_loss_q.backward()\n        optimizer_q.step()\n\n    if epoch % 50 == 0:\n        print(\n            f\"Epoch {epoch}: Discriminator Loss = {loss_t.item():.4f}, Generator Loss = {loss_q.item():.4f}, Contrast Loss = {contrast_loss.item():.4f}\"\n        )\n\n# After training, q_phi approximates the true posterior.", "cmd_opts": " --cell_id=NONE -s", "import_complete": 1, "terminal": "kitty"}