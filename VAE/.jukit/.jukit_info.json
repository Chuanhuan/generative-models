{"cmd": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nimport os\nimport imageio\n\n# 超参数与设备配置\nbatch_size = 100\nlatent_dim = 20\nepochs = 50\nnum_classes = 10\nimg_dim = 28\ninitial_filters = 16\nintermediate_dim = 256\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 数据加载\ntransform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Lambda(lambda x: x.view(1, img_dim, img_dim))]\n)\n\ntrain_dataset = datasets.MNIST(\n    \"~/Documents/data\", train=True, download=True, transform=transform\n)\ntest_dataset = datasets.MNIST(\"~/Documents/data\", train=False, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\nclass GaussianLayer(nn.Module):\n    def __init__(self, num_classes, latent_dim):\n        super().__init__()\n        self.num_classes = num_classes\n        self.latent_dim = latent_dim\n        self.mean = nn.Parameter(torch.zeros(num_classes, latent_dim))\n\n    def forward(self, z):\n        # 输入z形状: (batch_size, latent_dim)\n        # 输出形状: (batch_size, num_classes, latent_dim)\n        return z.unsqueeze(1) - self.mean.unsqueeze(0)\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            # 第一组卷积\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            # 第二组卷积\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n        )\n        self.flatten = nn.Flatten()\n        self.fc_mean = nn.Linear(64 * 7 * 7, latent_dim)\n        self.fc_logvar = nn.Linear(64 * 7 * 7, latent_dim)\n\n    def forward(self, x):\n        x = self.conv_layers(x)  # 输出形状: (batch, 64, 7, 7)\n        x = self.flatten(x)  # 形状: (batch, 64*7*7)\n        return self.fc_mean(x), self.fc_logvar(x)\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(latent_dim, 64 * 7 * 7)\n        self.conv_layers = nn.Sequential(\n            # 第一组转置卷积\n            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            # 第二组转置卷积\n            nn.ConvTranspose2d(\n                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),\n            nn.LeakyReLU(0.2),\n            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            # 第三组转置卷积\n            nn.ConvTranspose2d(\n                32, 1, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, z):\n        z = self.fc(z)\n        z = z.view(-1, 64, 7, 7)  # 重塑为卷积输入形状\n        return self.conv_layers(z)  # 输出形状: (batch, 1, 28, 28)\n\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, intermediate_dim),\n            nn.ReLU(),\n            nn.Linear(intermediate_dim, num_classes),\n            nn.Softmax(dim=1),\n        )\n\n    def forward(self, z):\n        return self.net(z)\n\n\nclass ClusterVAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.classifier = Classifier()\n        self.gaussian = GaussianLayer(num_classes, latent_dim)\n\n    def reparameterize(self, mean, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mean + eps * std\n\n    def forward(self, x):\n        # 编码\n        z_mean, z_logvar = self.encoder(x)\n        z = self.reparameterize(z_mean, z_logvar)\n\n        # 解码\n        x_recon = self.decoder(z)\n\n        # 分类\n        y = self.classifier(z)\n\n        # 高斯层处理\n        z_prior_mean = self.gaussian(z)  # 形状: (batch, num_classes, latent_dim)\n\n        return x_recon, z_prior_mean, y, z_mean, z_logvar\n\n\nmodel = ClusterVAE().to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# 训练循环\nlamb = 2.5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n\n        # 前向传播\n        x_recon, z_prior_mean, y, z_mean, z_logvar = model(data)\n\n        # 重构损失 (形状对齐)\n        recon_loss = 0.5 * F.mse_loss(x_recon, data, reduction=\"sum\") / data.size(0)\n\n        # KL散度损失 (维度对齐)\n        z_mean_exp = z_mean.unsqueeze(1)  # (batch, 1, latent_dim)\n        z_logvar_exp = z_logvar.unsqueeze(1)  # (batch, 1, latent_dim)\n\n        # 计算KL项: -0.5 * (logvar - (z_prior_mean)^2)\n        kl_elements = -0.5 * (z_logvar_exp - z_prior_mean.pow(2))\n\n        # 使用einsum进行张量乘积 (batch, num_classes) x (batch, num_classes, latent_dim)\n        kl_loss = torch.einsum(\"bi,bil->\", y, kl_elements).mean()\n\n        # 分类熵损失\n        cat_loss = -(y * torch.log(y + 1e-8)).sum(dim=1).mean()\n\n        # 总损失\n        loss = lamb * recon_loss + kl_loss + cat_loss\n\n        # 反向传播\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n\n\n# 评估函数\ndef cluster_accuracy(y_true, y_pred):\n    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n    for t, p in zip(y_true, y_pred):\n        conf_matrix[t, p] += 1\n    row_ind, col_ind = linear_sum_assignment(-conf_matrix)\n    return conf_matrix[row_ind, col_ind].sum() / len(y_true)\n\n\ndef evaluate(model, dataloader):\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for data, labels in dataloader:\n            data = data.to(device)\n            z_mean, _ = model.encoder(data)\n            y_probs = model.classifier(z_mean)\n            y_pred.extend(torch.argmax(y_probs, dim=1).cpu().numpy())\n            y_true.extend(labels.numpy())\n    return cluster_accuracy(y_true, y_pred)\n\n\n# 计算准确率\ntrain_acc = evaluate(model, train_loader)\ntest_acc = evaluate(model, test_loader)\nprint(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n\n# 生成样本函数\ndef generate_samples(model, path, category=0, num_samples=64):\n    model.eval()\n    with torch.no_grad():\n        # 获取类别中心\n        mean = model.gaussian.mean[category].unsqueeze(0).to(device)\n\n        # 生成样本\n        samples = []\n        for _ in range(num_samples):\n            z = mean + torch.randn(1, latent_dim).to(device)\n            sample = model.decoder(z).cpu().squeeze().numpy()\n            samples.append(sample)\n\n        # 创建网格\n        grid = np.zeros((img_dim * 8, img_dim * 8))\n        for i in range(8):\n            for j in range(8):\n                grid[\n                    i * img_dim : (i + 1) * img_dim, j * img_dim : (j + 1) * img_dim\n                ] = samples[i * 8 + j]\n\n        imageio.imwrite(path, (grid * 255).astype(np.uint8))\n\n\n# 保存样本\nif not os.path.exists(\"samples\"):\n    os.makedirs(\"samples\")\n\nfor c in range(num_classes):\n    generate_samples(model, f\"samples/class_{c}_samples.png\", category=c)", "cmd_opts": " --cell_id=NONE -s", "import_complete": 1, "terminal": "tmux"}